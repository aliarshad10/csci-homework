Final Paper:

Abstract:

This paper presents our aproach to navigate a Lego NXT robot through a dynamic environment. The robot must reach the end goal while avoiding contact with both moving and stationary obstacles. We combined data from a birds eye view webcam, and the onboard sonar, light and touch sensors. We considered 5 possible moves for each location of the robot and our controler algorithm would choose the move that would minimize the distance to the goal while avoiding obstacles. Our final implementation was a greedy on policy approach which worked for more than half of the trials. Learning from this project we will discuss possible changes to our approach for future implementations.

Introduction:

The task at hand involved programming a robot to navigate to a goal ontop of a table while avoiding obstacles and the edge. The system had to be responsive to changes in the environment, as the obstacles we not necessarily stationary. We incorperated machine learning techniques to achive a flexible and robust solution that would improve as the robot moved towards the goal. Sensors we incorperated were a standard definition web cam, utlrasonic distance sensor, light sensor and touch sensor. The camera was placed over the table, to determine the possition and orientation of the robot, the goal, and drivable regions. The light and touch sensors on the NXT were our backup sensors for avoiding the table edge and objects, respectively. Our ultrasonic sensor was mounted on a servomotor to allow us to check multiple directions and incorperate the different distance messures into our overall controller.

Platform:

Our robot was a Lego NXT tribot, we made two notible modifications, first mounting a third sevromotor to allow the forward facing ultrasonic sensor to rotate. Second we mounted coloured blobs above the front and rear of the robot to simplify the detection and calculate the direction of the robot. Our project relied on Python, nxt-python and OpenCV (versions 2.7.2, 2.2, 2.4.3 respectively). We developed and tested our software across Linux, Windows and Mac with different webcams without experiencing any added difficulty. Scikit-Learn was consided, to learn a function to translate from camera data to the robot's orientation; this option was determined unnessicary because we were able to calculate the direction of the robot analytically with triganometry.

Controller Achitecture:

We devided our overall solution into smaller tasks, which would work well in parallel. Our aim was to have all of the processes running in parallel and then combine them all, to decide which action to take given the robots current percieved location. We also include past data and the robots history through a PID controller. In our final implementation, our sub processes ran a psuedo parallel manor. The processes ran in sequence, but independently of each other; there results were all taken into account after they returned as if they had all ran in parallel. This decission was made due to time constraints and not knowing the ins and outs of python multi-processing. We can assume that a purely parallel implementation would indeed be faster; at the beginning of each step, the robot locates itself which requires grabbing several frames from the webcame discarding the first few because the camera had autofocus and colour filters that we were unable to turn off. If we could run the vision completely in parallel, our vision task would constantly be running, any the location and orientation of the robot could be queried at any time without waiting for the camera to re adjust.

Perception:

The OpenCV library was used to interface with the webcam. The latest Major build of OpenCV stored image data as numpy arrays. This allows for simplified processing and and iterating through the image data to find values matching a certain criteria. All images were blurred with guassian filters before any other image processing took place. First we selected four areas of the image, the front and back colours on the robot, the colour of the goal and the colour of the table. This took a small average of Hue Saturation and Value values in the pixels surounding the users selected area, we set tollerance window to accept values within range of this average when later searching for areas of the image that matched. Instead of looking for objects to avoid, and the edge of the table, we searched for the table. To determine if an area is safe to move, we check if most of that area matches the HSV values of the table's surface. 

Object Avoidence:

We had two levels of Object avoidence implemented and they were both taken into acount when our robot decided what move to execute next. The first method used computer vision. the first method would trace straight lines; one from the robot towards the goal, and the others ofset away from the first at set angles. We used  a line tracing algorithm found in ``a Vectorial Algorithm for Tracing Straight Lines in N-Dimensional Generalized Grids''. Moving outward from the robot we determined how far in that direction the robot would be able to continue while avoiding obstacles by checking if each pixel the line matched the table's values within tollerance.

We also used an utlrasonic distance sensor mounted on a sevromotor to check five different directions relative to the robot. We checked forward, and twice to each side with a set angle between each similar to the vision based obstacle avoidance. This was planned to be used as our back up method of avoiding obstacles but turned out to be more effective, so when the values are combined we account for the stronger belief in the sonars.

Learning

Our approach is based on on-policy policy iteration, but we've added a greedy twist for computational simplicity. We do not iterate over every possible policy in all possible states. We Consider the state the robot is currently located in. The robot will attempt to exicute a move to improve it's state, where the reward is distance to the goal, but only straight line distance to the goal, not avoiding future objects. Our policies are five directions from the current position the robot could go. The actual motor movements are all smoothed by a Proportional-Integral-Derivative (PID) controller. We do not iterate over all states saving the optimal policies for each state because the obstacles are not static, and we would need to recalculate all the policies each time an obstacle moved. We also wanted to avoid any training time, we wanted a robot that would work out of the box, and improve with time. Our robot does not have any movement commands hardcoded, and it will work if the motor cables are swapped. The robot attemptes a motor command and recieves feedback when the vision sensor values are updated; if the move was not benificial the error in either distance or angle would have increased. When the robot tries to turn left but the change in angle moves in the wrong direction, the PID controller will compensate for this and the robot will turn in the correct direction.

Future Improvements:

Several future improvements could be made, We'll discuss three posible future enhancements. First, add a more robust path planning algorithm. We went for keeping it simple. In our environment, we knew our greedy aproach would work fast enough because of the simple box obstacles we needed to avoid. In a more complex maze environment is it possible for our algorithm to fail and our robot to get stuck. Second, separate the code to a completely parallel implementation. It would be nice to have the vision processes, path planning and object avoidance all running as seperate processes. This could be implemented by using the libraries provided by the Robot Operating System (ROS) to pass data between different processes in a simple manor. The communicaton infrastructure provided by ROS would allow for the vision do be running continuosly publishing the robots location, while the Path planning, Object avoiding and line tracing all run concurrently instead of in sequence. Finally it would be benifical to add more machine learning methods, or flexible controller archatecture where we could swap controllers and test their direct differences and compare directly different machine learning methods to decide the best. For our project, we choose the one that would work with our time constraints. 

Conclusion:

We presented our project and our approach and were pleased with our results, in the two runs we reached the goal on the first trial, and not on the second trial. In our final dozen tests, we managed to reach the goal in three quarters of the tests. We know our robot worked starting in any starting position and we were happy with our results in trials leading upto the final presentation. We were happy with the simplicity in our solution. It proved to be more robust than other methods that relied on large training sets and we did not rely on hard code any camera transforms correct the skewing around the edges of the image. Our simple solution was designed to be robust and easy to use anywhere on any machine in as little time possible. 

References:

We would like to thank Jeff for allowing us to use his hsv click to select code. aka the 'clicky-code'. soon to be open source. https://github.com/Linthalor

