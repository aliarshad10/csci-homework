\section{Machine Learning}
\label{section::learning}

This approach is based on on-policy policy iteration, but a greedy twist was added for computational simplicity. We do not iterate over every possible policy in all possible states. The algorithm considers the current state of the robot. The robot will attempt to execute a move to improve its state, where the reward is the straight line distance to the goal without avoiding future objects. Five policies were set from the current position the robot could go. The actual motor movements are all smoothed by a PID controller. The algorithm does not iterate over all states saving the optimal policies for each state because the obstacles are not static, and the policies would need to be recalculated each time an obstacle moved. A decision was made to avoid any training time in order for the robot to work out of the box, and improve with time. The robot does not have any movement commands hardcoded, and it will work if the motor cables are swapped. The robot attempts a motor command and receives feedback when the vision sensor values are updated; if the move was not beneficial, the error in either distance or angle would have increased. When the robot tries to turn left but the change in angle moves in the wrong direction, the PID controller will compensate for this and the robot will turn in the correct direction.
